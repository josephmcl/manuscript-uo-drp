%
%
%
\section{EXPERIMENTS}

%
%
%
In this section we complement our analysis with performance results of our parallel implementation of the 2D hybridized SBP-SAT problem.
We developed code used to solve this system in C++ and utilize the PETSc library to implement the linear algebra operations utilized throughout. 
All linear systems are solved using the direct LU solver provided by PETSc.
We implemented thread parallel routines in OpenMP \citep{openmp08}, utilizing the parallelism made available through hybridization.
Our code ran on the University of Oregon's Talapas supercomputing cluster on an Intel Xeon E5-2690 v4 CPU. 
Profiling information including cache accesses, writes, and flop counts were sought using the PAPI profiler \citep{jagode2019papi}.

%
%
%
\subsection{Strong scaling}
\input{text/Scaling_Analysis.tex}


\subsection{SpMV analysis}
The SpMV kernel is similarly batched to compute $\textbf{F}^\intercal \times (\textbf{M}^{-1}_{i} \bar{\textbf{g}})$ for each element. 
Unlike MatMul, each intermediate vector in $\textbf{M}^{-1}_{i} \bar{\textbf{g}}$ is unique since $\bar{\textbf{g}}$ is unique. 
Additionally, we compute proportionally fewer batches of $n^2 \times n$ SpMV than MatMul, and write out to significantly fewer memory locations. 
We still encounter the same write miss penalty as we both increase the size of the intermediate vector $\textbf{F}^\intercal \times (\textbf{M}^{-1}_{i} \bar{\textbf{g}})$ and decrease the size of each batch operation proportionally.
Even at the smallest element size each batch write 21 doubles, which is larger than a typical cache line. 

%
%
%
\subsection{Memory behavior}

%
%
%
To investigate the decreasing performance of our MatMul and SpMV operations we chose pairs of $\bar{n}$ and $\ell$ with similar total flops for the MatMul kernel.
These problems averaged $1.32\mathrm{e}11$ flops with a coefficient of variation of $2.3\mathrm{e}{-3}$.
Though the flops between each problem varies insignificantly, and the bytes loa\-ded \emph{decrease}, the number of entries in $\symbf{\lambda_{\textbf{A}}}$ --- and hence the number of bytes written \emph{increases}. 
This is illustrated in on the left of \figref{fig:mem_exp_b}; this also plots the measured number of L3 cache writes in MatMul kernel on the right. 
The number of L3 writes grows proportional to the number of entries in $\symbf{\lambda_{\textbf{A}}}$.
The number of entries are approximately $80\times$ greater than the number of L3 writes. This is slightly less than the number of 8 byte doubles that fit into a 128 byte cache line.

%
%
%
\begin{figure}[!h]
	\input{./figure/mem_exp_2.tex}
	\caption{
	    The amount of non-zero elements in $\symbf{\lambda_{\textbf{A}}}$ for a set of problems with similar flop profiles (left) and the number of L3 cache data writes measured with PAPI (right). 
	    While the total flops are similar, the size and number of entries in $\symbf{\lambda_{\textbf{A}}}$ continues to grow. 
	    }
    \label{fig:mem_exp_b}
\end{figure}

\noindent
The runtime of MatMul SpMV, and the LU factorization of $\symbf{\lambda}_{\textbf{A}}$ all increase in runtime, even though the flops remaining minimally changed and the bytes loaded decrease. 
We illustrate this in \figref{fig:mem_exp_a}. 
The growth of the $\symbf{\lambda}_{\textbf{A}}$ factorization ide to the additional non-zero entries increasing its complexity.
For the other two operations this suggests that the greatest impact to runtime at this scale is the number of writes to $\symbf{\lambda}_{\textbf{A}}$, \emph{i.e.}, that these operations are becoming write bound.
This is affirmed by \figref{fig:mmwm}, as additional the sum of L3 write and misses in the MatMul kernel as we increase the number of elements while fixing grid points.  


\begin{figure}[h]
	\input{./figure/mem_exp_1.tex}
	\caption{
	    The runtime of three expensive operations using 28 threads on several problem sizes with a similar total flops. 
	    Though the total flops remain essentially the same, the runtime increases for all three kernels.
	    }
	\label{fig:mem_exp_a}
\end{figure}



%
%
%



%\begin{mcfigure}
%	\centering
%	\input{./figure/total_gflops.tex}
%	\captionof{figure}{Total FLOPs used to compute the factorized matrix multiplication in $\textbf{F}^{\intercal} \times (\textbf{M}^{-1} \textbf{F})$ over the number of elements for a constant problem size, $N = 705\kern 0.25em 600$. FLOPs decrease as the size of the constituent inner products decrease in relation to the size of each element.}
%\end{mcfigure}



%\subsection{Delta Solve Analysis}

%\begin{mcfigure}
%	\centering
%	\input{./figure/serial_tts_trunc.tex}
%\end{mcfigure}

